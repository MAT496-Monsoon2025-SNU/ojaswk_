{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T18:17:38.421062Z",
     "start_time": "2025-09-12T18:17:38.418823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!pip install langchain_community\n",
    "#!pip install langchain_anthropic\n",
    "#!pip install langchain_huggingface langchain_chroma\n",
    "#!pip install sentence_transformers langchain_openai\n",
    "#!pip install pypdf\n",
    "# !pip install langchain_openai"
   ],
   "id": "86606c2acb61b3f",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T09:37:24.128310Z",
     "start_time": "2025-09-16T09:37:13.079583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader, WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ],
   "id": "84133e30ad252a19",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T09:37:26.285784Z",
     "start_time": "2025-09-16T09:37:26.278371Z"
    }
   },
   "cell_type": "code",
   "source": "load_dotenv()",
   "id": "a29bad4764c69810",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T09:37:32.328547Z",
     "start_time": "2025-09-16T09:37:32.292274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiProviderRAG:\n",
    "    def __init__(self,\n",
    "                 provider=\"groq\",  # \"groq\" or \"claude\"\n",
    "                 model_configs=None,\n",
    "                 embedding_model=\"text-embedding-3-small\",\n",
    "                 chunk_size=1200,\n",
    "                 chunk_overlap=200,\n",
    "                 collection_name=\"documents\"):\n",
    "\n",
    "        # Default model configurations\n",
    "        if model_configs is None:\n",
    "            model_configs = {\n",
    "                \"groq\": {\n",
    "                    \"model\": \"llama-3.1-70b-versatile\",\n",
    "                    \"temperature\": 0.1\n",
    "                },\n",
    "                \"claude\": {\n",
    "                    \"model\": \"claude-3-haiku-20240307\",\n",
    "                    \"temperature\": 0.2\n",
    "                }\n",
    "            }\n",
    "\n",
    "        self.provider = provider\n",
    "        self.model_configs = model_configs\n",
    "\n",
    "        # Initialize the selected LLM\n",
    "        self.llm = self._initialize_llm(provider)\n",
    "\n",
    "        # Use OpenAI embeddings (consistent across providers)\n",
    "        self.embeddings = OpenAIEmbeddings(\n",
    "            model=embedding_model,\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "        )\n",
    "\n",
    "        # Configure text splitter\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \"? \", \"! \", \" \", \"\"],\n",
    "            add_start_index=True\n",
    "        )\n",
    "\n",
    "        self.collection_name = collection_name\n",
    "        self.vector_store = None\n",
    "        self.retriever = None\n",
    "\n",
    "        # Provider-specific prompt templates\n",
    "        self.prompt_templates = {\n",
    "            \"groq\": ChatPromptTemplate.from_template(\"\"\"\n",
    "            You are a helpful AI assistant analyzing documents. Answer the question based on the provided context.\n",
    "\n",
    "            Be direct and informative:\n",
    "            - Use the context to provide accurate answers\n",
    "            - Quote relevant passages when helpful\n",
    "            - If context is insufficient, explain what's missing\n",
    "            - Keep responses focused and practical\n",
    "\n",
    "            Context:\n",
    "            {context}\n",
    "\n",
    "            Question: {question}\n",
    "\n",
    "            Answer:\n",
    "            \"\"\"),\n",
    "\n",
    "            \"claude\": ChatPromptTemplate.from_template(\"\"\"\n",
    "            You are a thoughtful document analyst. Provide a comprehensive answer based on the given context.\n",
    "\n",
    "            Guidelines:\n",
    "            - Analyze the context thoroughly before responding\n",
    "            - Provide nuanced insights where appropriate\n",
    "            - Reference specific details from the documents\n",
    "            - Acknowledge uncertainty when context is limited\n",
    "            - Structure your response clearly\n",
    "\n",
    "            Document Context:\n",
    "            {context}\n",
    "\n",
    "            Question: {question}\n",
    "\n",
    "            Analysis:\n",
    "            \"\"\")\n",
    "        }\n",
    "\n",
    "    def _initialize_llm(self, provider):\n",
    "        \"\"\"Initialize LLM based on provider choice\"\"\"\n",
    "        config = self.model_configs[provider]\n",
    "\n",
    "        if provider == \"groq\":\n",
    "            return ChatGroq(\n",
    "                model=config[\"model\"],\n",
    "                api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "                temperature=config[\"temperature\"]\n",
    "            )\n",
    "        elif provider == \"claude\":\n",
    "            return ChatAnthropic(\n",
    "                model=config[\"model\"],\n",
    "                api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "                temperature=config[\"temperature\"]\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported provider: {provider}\")\n",
    "\n",
    "    def switch_provider(self, new_provider):\n",
    "        \"\"\"Switch between Groq and Claude\"\"\"\n",
    "        if new_provider not in [\"groq\", \"claude\"]:\n",
    "            raise ValueError(\"Provider must be 'groq' or 'claude'\")\n",
    "\n",
    "        self.provider = new_provider\n",
    "        self.llm = self._initialize_llm(new_provider)\n",
    "        print(f\"Switched to {new_provider} ({self.model_configs[new_provider]['model']})\")\n",
    "\n",
    "    def load_documents(self, source, source_type=\"pdf\"):\n",
    "        \"\"\"Load documents from various sources\"\"\"\n",
    "        if source_type == \"pdf\":\n",
    "            if isinstance(source, str):\n",
    "                source = [source]\n",
    "\n",
    "            all_documents = []\n",
    "            for file_path in source:\n",
    "                loader = PyPDFLoader(file_path)\n",
    "                documents = loader.load()\n",
    "                all_documents.extend(documents)\n",
    "                print(f\"Loaded {len(documents)} pages from {file_path}\")\n",
    "\n",
    "            return all_documents\n",
    "\n",
    "        elif source_type == \"web\":\n",
    "            if isinstance(source, str):\n",
    "                source = [source]\n",
    "\n",
    "            loader = WebBaseLoader(source)\n",
    "            documents = loader.load()\n",
    "            print(f\"Loaded {len(documents)} web documents\")\n",
    "            return documents\n",
    "\n",
    "        elif source_type == \"directory\":\n",
    "            loader = DirectoryLoader(\n",
    "                source,\n",
    "                glob=\"**/*.pdf\",\n",
    "                loader_cls=PyPDFLoader,\n",
    "                show_progress=True\n",
    "            )\n",
    "            documents = loader.load()\n",
    "            print(f\"Loaded {len(documents)} documents from directory\")\n",
    "            return documents\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"source_type must be 'pdf', 'web', or 'directory'\")\n",
    "\n",
    "    def setup_vector_store(self, documents, persist_directory=\"./rag_db\"):\n",
    "        \"\"\"Create vector database\"\"\"\n",
    "        text_chunks = self.text_splitter.split_documents(documents)\n",
    "        print(f\"Created {len(text_chunks)} text chunks\")\n",
    "\n",
    "        self.vector_store = Chroma.from_documents(\n",
    "            documents=text_chunks,\n",
    "            embedding=self.embeddings,\n",
    "            collection_name=self.collection_name,\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "\n",
    "        self.retriever = self.vector_store.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 6}\n",
    "        )\n",
    "\n",
    "        return len(text_chunks)\n",
    "\n",
    "    def load_vector_store(self, persist_directory=\"./rag_db\"):\n",
    "        \"\"\"Load existing vector database\"\"\"\n",
    "        self.vector_store = Chroma(\n",
    "            collection_name=self.collection_name,\n",
    "            embedding_function=self.embeddings,\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "\n",
    "        self.retriever = self.vector_store.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 6}\n",
    "        )\n",
    "\n",
    "        print(\"Loaded existing vector database\")\n",
    "\n",
    "    def query(self, question):\n",
    "        \"\"\"Query documents using current provider\"\"\"\n",
    "        if not self.retriever:\n",
    "            raise ValueError(\"Vector store not initialized\")\n",
    "\n",
    "        # Get provider-specific prompt\n",
    "        prompt_template = self.prompt_templates[self.provider]\n",
    "\n",
    "        # Create analysis chain\n",
    "        chain = (\n",
    "            {\n",
    "                \"context\": self.retriever | RunnableLambda(self._format_context),\n",
    "                \"question\": RunnablePassthrough()\n",
    "            }\n",
    "            | prompt_template\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        return chain.invoke(question)\n",
    "\n",
    "    def compare_providers(self, question):\n",
    "        \"\"\"Compare responses from both providers\"\"\"\n",
    "        if not self.retriever:\n",
    "            raise ValueError(\"Vector store not initialized\")\n",
    "\n",
    "        results = {}\n",
    "        original_provider = self.provider\n",
    "\n",
    "        for provider in [\"groq\", \"claude\"]:\n",
    "            try:\n",
    "                self.switch_provider(provider)\n",
    "                response = self.query(question)\n",
    "                results[provider] = {\n",
    "                    \"model\": self.model_configs[provider][\"model\"],\n",
    "                    \"response\": response\n",
    "                }\n",
    "            except Exception as e:\n",
    "                results[provider] = {\n",
    "                    \"model\": self.model_configs[provider][\"model\"],\n",
    "                    \"response\": f\"Error: {str(e)}\"\n",
    "                }\n",
    "\n",
    "        # Restore original provider\n",
    "        self.switch_provider(original_provider)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _format_context(self, documents):\n",
    "        \"\"\"Format retrieved documents\"\"\"\n",
    "        if not documents:\n",
    "            return \"No relevant documents found.\"\n",
    "\n",
    "        formatted = []\n",
    "        for i, doc in enumerate(documents, 1):\n",
    "            source = doc.metadata.get('source', 'Unknown')\n",
    "            page = doc.metadata.get('page', 'N/A')\n",
    "\n",
    "            formatted.append(\n",
    "                f\"[Doc {i} - {source}, p.{page}]\\n{doc.page_content}\\n\"\n",
    "            )\n",
    "\n",
    "        return \"\\n---\\n\".join(formatted)\n",
    "\n",
    "    def get_system_info(self):\n",
    "        \"\"\"Get current system configuration\"\"\"\n",
    "        return {\n",
    "            \"current_provider\": self.provider,\n",
    "            \"current_model\": self.model_configs[self.provider][\"model\"],\n",
    "            \"available_providers\": list(self.model_configs.keys()),\n",
    "            \"vector_store_loaded\": self.vector_store is not None,\n",
    "            \"embedding_model\": \"text-embedding-3-small\"\n",
    "        }\n",
    "\n",
    "# Usage example\n",
    "def demo():\n",
    "    # Initialize with Groq as default\n",
    "    rag = MultiProviderRAG(\n",
    "        provider=\"groq\",\n",
    "        model_configs={\n",
    "            \"groq\": {\n",
    "                \"model\": \"llama-3.1-70b-versatile\",\n",
    "                \"temperature\": 0.1\n",
    "            },\n",
    "            \"claude\": {\n",
    "                \"model\": \"claude-3-haiku-20240307\",\n",
    "                \"temperature\": 0.2\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Load documents\n",
    "        documents = rag.load_documents([\"sample.pdf\"], source_type=\"pdf\")\n",
    "\n",
    "        # Create vector store\n",
    "        rag.setup_vector_store(documents, \"./multi_provider_db\")\n",
    "\n",
    "        # Test question\n",
    "        question = \"What are the main topics covered in these documents?\"\n",
    "\n",
    "        # Query with Groq\n",
    "        print(\"=== GROQ RESPONSE ===\")\n",
    "        groq_response = rag.query(question)\n",
    "        print(groq_response)\n",
    "\n",
    "        # Switch to Claude\n",
    "        rag.switch_provider(\"claude\")\n",
    "        print(\"\\n=== CLAUDE RESPONSE ===\")\n",
    "        claude_response = rag.query(question)\n",
    "        print(claude_response)\n",
    "\n",
    "        # Compare both providers\n",
    "        print(\"\\n=== PROVIDER COMPARISON ===\")\n",
    "        comparison = rag.compare_providers(\"Summarize the key findings\")\n",
    "\n",
    "        for provider, result in comparison.items():\n",
    "            print(f\"\\n{provider.upper()} ({result['model']}):\")\n",
    "            print(result['response'])\n",
    "\n",
    "        # Show system info\n",
    "        print(f\"\\nSystem Info: {rag.get_system_info()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"\\nRequired environment variables:\")\n",
    "        print(\"- GROQ_API_KEY\")\n",
    "        print(\"- ANTHROPIC_API_KEY\")\n",
    "        print(\"- OPENAI_API_KEY (for embeddings)\")"
   ],
   "id": "9c6d5fee8d6b2d30",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T09:37:37.427158Z",
     "start_time": "2025-09-16T09:37:37.419103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def demo():\n",
    "    # Initialize with Groq as default\n",
    "    rag = MultiProviderRAG(\n",
    "        provider=\"groq\",\n",
    "        model_configs={\n",
    "            \"groq\": {\n",
    "                \"model\": \"llama-3.1-70b-versatile\",\n",
    "                \"temperature\": 0.1\n",
    "            },\n",
    "            \"claude\": {\n",
    "                \"model\": \"claude-3-haiku-20240307\",\n",
    "                \"temperature\": 0.2\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Load documents\n",
    "        documents = rag.load_documents([\"sample.pdf\"], source_type=\"pdf\")\n",
    "\n",
    "        # Create vector store\n",
    "        rag.setup_vector_store(documents, \"./multi_provider_db\")\n",
    "\n",
    "        # Test question\n",
    "        question = \"What are the main topics covered in these documents?\"\n",
    "\n",
    "        # Query with Groq\n",
    "        print(\"=== GROQ RESPONSE ===\")\n",
    "        groq_response = rag.query(question)\n",
    "        print(groq_response)\n",
    "\n",
    "        # Switch to Claude\n",
    "        rag.switch_provider(\"claude\")\n",
    "        print(\"\\n=== CLAUDE RESPONSE ===\")\n",
    "        claude_response = rag.query(question)\n",
    "        print(claude_response)\n",
    "\n",
    "        # Compare both providers\n",
    "        print(\"\\n=== PROVIDER COMPARISON ===\")\n",
    "        comparison = rag.compare_providers(\"Summarize the key findings\")\n",
    "\n",
    "        for provider, result in comparison.items():\n",
    "            print(f\"\\n{provider.upper()} ({result['model']}):\")\n",
    "            print(result['response'])\n",
    "\n",
    "        # Show system info\n",
    "        print(f\"\\nSystem Info: {rag.get_system_info()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"\\nRequired environment variables:\")\n",
    "        print(\"- GROQ_API_KEY\")\n",
    "        print(\"- ANTHROPIC_API_KEY\")\n",
    "        print(\"- OPENAI_API_KEY (for embeddings)\")"
   ],
   "id": "bdb10a124cf609a1",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T09:37:41.437740Z",
     "start_time": "2025-09-16T09:37:40.770435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    demo()"
   ],
   "id": "e48b81812bd3892f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: File path sample.pdf is not a valid file or url\n",
      "\n",
      "Required environment variables:\n",
      "- GROQ_API_KEY\n",
      "- ANTHROPIC_API_KEY\n",
      "- OPENAI_API_KEY (for embeddings)\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
