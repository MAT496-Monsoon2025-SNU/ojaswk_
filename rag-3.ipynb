{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T18:17:38.421062Z",
     "start_time": "2025-09-12T18:17:38.418823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!pip install langchain_community\n",
    "#!pip install langchain_anthropic\n",
    "#!pip install langchain_huggingface langchain_chroma\n",
    "#!pip install sentence_transformers langchain_openai\n",
    "#!pip install pypdf\n",
    "# !pip install langchain_openai"
   ],
   "id": "86606c2acb61b3f",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T09:37:24.128310Z",
     "start_time": "2025-09-16T09:37:13.079583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader, WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ],
   "id": "84133e30ad252a19",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T09:37:26.285784Z",
     "start_time": "2025-09-16T09:37:26.278371Z"
    }
   },
   "cell_type": "code",
   "source": "load_dotenv()",
   "id": "a29bad4764c69810",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T09:50:39.515827Z",
     "start_time": "2025-09-16T09:50:39.503135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiProviderRAG:\n",
    "    def __init__(self,\n",
    "                 provider=\"groq\",  # \"groq\" or \"claude\"\n",
    "                 model_configs=None,\n",
    "                 embedding_model=\"text-embedding-3-small\",\n",
    "                 chunk_size=1200,\n",
    "                 chunk_overlap=200,\n",
    "                 collection_name=\"documents\"):\n",
    "\n",
    "        # Default model configurations\n",
    "        if model_configs is None:\n",
    "            model_configs = {\n",
    "                \"groq\": {\n",
    "                    \"model\": \"llama-3.1-70b-versatile\",\n",
    "                    \"temperature\": 0.1\n",
    "                },\n",
    "                \"claude\": {\n",
    "                    \"model\": \"claude-3-haiku-20240307\",\n",
    "                    \"temperature\": 0.2\n",
    "                }\n",
    "            }\n",
    "\n",
    "        self.provider = provider\n",
    "        self.model_configs = model_configs\n",
    "\n",
    "        # Initialize the selected LLM\n",
    "        self.llm = self._initialize_llm(provider)\n",
    "\n",
    "        # Use OpenAI embeddings (consistent across providers)\n",
    "        self.embeddings = OpenAIEmbeddings(\n",
    "            model=embedding_model,\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "        )\n",
    "\n",
    "        # Configure text splitter\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \"? \", \"! \", \" \", \"\"],\n",
    "            add_start_index=True\n",
    "        )\n",
    "\n",
    "        self.collection_name = collection_name\n",
    "        self.vector_store = None\n",
    "        self.retriever = None\n",
    "\n",
    "        # Provider-specific prompt templates\n",
    "        self.prompt_templates = {\n",
    "            \"groq\": ChatPromptTemplate.from_template(\"\"\"\n",
    "            You are a helpful AI assistant analyzing documents. Answer the question based on the provided context.\n",
    "\n",
    "            Be direct and informative:\n",
    "            - Use the context to provide accurate answers\n",
    "            - Quote relevant passages when helpful\n",
    "            - If context is insufficient, explain what's missing\n",
    "            - Keep responses focused and practical\n",
    "\n",
    "            Context:\n",
    "            {context}\n",
    "\n",
    "            Question: {question}\n",
    "\n",
    "            Answer:\n",
    "            \"\"\"),\n",
    "\n",
    "            \"claude\": ChatPromptTemplate.from_template(\"\"\"\n",
    "            You are a thoughtful document analyst. Provide a comprehensive answer based on the given context.\n",
    "\n",
    "            Guidelines:\n",
    "            - Analyze the context thoroughly before responding\n",
    "            - Provide nuanced insights where appropriate\n",
    "            - Reference specific details from the documents\n",
    "            - Acknowledge uncertainty when context is limited\n",
    "            - Structure your response clearly\n",
    "\n",
    "            Document Context:\n",
    "            {context}\n",
    "\n",
    "            Question: {question}\n",
    "\n",
    "            Analysis:\n",
    "            \"\"\")\n",
    "        }\n",
    "\n",
    "    def _initialize_llm(self, provider):\n",
    "        \"\"\"Initialize LLM based on provider choice\"\"\"\n",
    "        config = self.model_configs[provider]\n",
    "\n",
    "        if provider == \"groq\":\n",
    "            return ChatGroq(\n",
    "                model=config[\"model\"],\n",
    "                api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "                temperature=config[\"temperature\"]\n",
    "            )\n",
    "        elif provider == \"claude\":\n",
    "            return ChatAnthropic(\n",
    "                model=config[\"model\"],\n",
    "                api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "                temperature=config[\"temperature\"]\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported provider: {provider}\")\n",
    "\n",
    "    def switch_provider(self, new_provider):\n",
    "        \"\"\"Switch between Groq and Claude\"\"\"\n",
    "        if new_provider not in [\"groq\", \"claude\"]:\n",
    "            raise ValueError(\"Provider must be 'groq' or 'claude'\")\n",
    "\n",
    "        self.provider = new_provider\n",
    "        self.llm = self._initialize_llm(new_provider)\n",
    "        print(f\"Switched to {new_provider} ({self.model_configs[new_provider]['model']})\")\n",
    "\n",
    "    def load_documents(self, source, source_type=\"pdf\"):\n",
    "        \"\"\"Load documents from various sources\"\"\"\n",
    "        if source_type == \"pdf\":\n",
    "            if isinstance(source, str):\n",
    "                source = [source]\n",
    "\n",
    "            all_documents = []\n",
    "            for file_path in source:\n",
    "                loader = PyPDFLoader(file_path)\n",
    "                documents = loader.load()\n",
    "                all_documents.extend(documents)\n",
    "                print(f\"Loaded {len(documents)} pages from {file_path}\")\n",
    "\n",
    "            return all_documents\n",
    "\n",
    "        elif source_type == \"web\":\n",
    "            if isinstance(source, str):\n",
    "                source = [source]\n",
    "\n",
    "            loader = WebBaseLoader(source)\n",
    "            documents = loader.load()\n",
    "            print(f\"Loaded {len(documents)} web documents\")\n",
    "            return documents\n",
    "\n",
    "        elif source_type == \"directory\":\n",
    "            loader = DirectoryLoader(\n",
    "                source,\n",
    "                glob=\"**/*.pdf\",\n",
    "                loader_cls=PyPDFLoader,\n",
    "                show_progress=True\n",
    "            )\n",
    "            documents = loader.load()\n",
    "            print(f\"Loaded {len(documents)} documents from directory\")\n",
    "            return documents\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"source_type must be 'pdf', 'web', or 'directory'\")\n",
    "\n",
    "    def setup_vector_store(self, documents, persist_directory=\"./rag_db\"):\n",
    "        \"\"\"Create vector database\"\"\"\n",
    "        text_chunks = self.text_splitter.split_documents(documents)\n",
    "        print(f\"Created {len(text_chunks)} text chunks\")\n",
    "\n",
    "        self.vector_store = Chroma.from_documents(\n",
    "            documents=text_chunks,\n",
    "            embedding=self.embeddings,\n",
    "            collection_name=self.collection_name,\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "\n",
    "        self.retriever = self.vector_store.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 6}\n",
    "        )\n",
    "\n",
    "        return len(text_chunks)\n",
    "\n",
    "    def load_vector_store(self, persist_directory=\"./rag_db\"):\n",
    "        \"\"\"Load existing vector database\"\"\"\n",
    "        self.vector_store = Chroma(\n",
    "            collection_name=self.collection_name,\n",
    "            embedding_function=self.embeddings,\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "\n",
    "        self.retriever = self.vector_store.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 6}\n",
    "        )\n",
    "\n",
    "        print(\"Loaded existing vector database\")\n",
    "\n",
    "    def query(self, question):\n",
    "        \"\"\"Query documents using current provider\"\"\"\n",
    "        if not self.retriever:\n",
    "            raise ValueError(\"Vector store not initialized\")\n",
    "\n",
    "        # Get provider-specific prompt\n",
    "        prompt_template = self.prompt_templates[self.provider]\n",
    "\n",
    "        # Create analysis chain\n",
    "        chain = (\n",
    "            {\n",
    "                \"context\": self.retriever | RunnableLambda(self._format_context),\n",
    "                \"question\": RunnablePassthrough()\n",
    "            }\n",
    "            | prompt_template\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        return chain.invoke(question)\n",
    "\n",
    "    def compare_providers(self, question):\n",
    "        \"\"\"Compare responses from both providers\"\"\"\n",
    "        if not self.retriever:\n",
    "            raise ValueError(\"Vector store not initialized\")\n",
    "\n",
    "        results = {}\n",
    "        original_provider = self.provider\n",
    "\n",
    "        for provider in [\"groq\", \"claude\"]:\n",
    "            try:\n",
    "                self.switch_provider(provider)\n",
    "                response = self.query(question)\n",
    "                results[provider] = {\n",
    "                    \"model\": self.model_configs[provider][\"model\"],\n",
    "                    \"response\": response\n",
    "                }\n",
    "            except Exception as e:\n",
    "                results[provider] = {\n",
    "                    \"model\": self.model_configs[provider][\"model\"],\n",
    "                    \"response\": f\"Error: {str(e)}\"\n",
    "                }\n",
    "\n",
    "        # Restore original provider\n",
    "        self.switch_provider(original_provider)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _format_context(self, documents):\n",
    "        \"\"\"Format retrieved documents\"\"\"\n",
    "        if not documents:\n",
    "            return \"No relevant documents found.\"\n",
    "\n",
    "        formatted = []\n",
    "        for i, doc in enumerate(documents, 1):\n",
    "            source = doc.metadata.get('source', 'Unknown')\n",
    "            page = doc.metadata.get('page', 'N/A')\n",
    "\n",
    "            formatted.append(\n",
    "                f\"[Doc {i} - {source}, p.{page}]\\n{doc.page_content}\\n\"\n",
    "            )\n",
    "\n",
    "        return \"\\n---\\n\".join(formatted)\n",
    "\n",
    "    def get_system_info(self):\n",
    "        \"\"\"Get current system configuration\"\"\"\n",
    "        return {\n",
    "            \"current_provider\": self.provider,\n",
    "            \"current_model\": self.model_configs[self.provider][\"model\"],\n",
    "            \"available_providers\": list(self.model_configs.keys()),\n",
    "            \"vector_store_loaded\": self.vector_store is not None,\n",
    "            \"embedding_model\": \"text-embedding-3-small\"\n",
    "        }\n",
    "\n",
    "# Usage example\n",
    "def demo():\n",
    "    # Initialize with Groq as default\n",
    "    rag = MultiProviderRAG(\n",
    "        provider=\"groq\",\n",
    "        model_configs={\n",
    "            \"groq\": {\n",
    "                \"model\": \"llama-3.1-70b-versatile\",\n",
    "                \"temperature\": 0.1\n",
    "            },\n",
    "            \"claude\": {\n",
    "                \"model\": \"claude-3-haiku-20240307\",\n",
    "                \"temperature\": 0.2\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Load documents\n",
    "        documents = rag.load_documents([\"/home/ojas/Downloads/sample.pdf\"], source_type=\"pdf\")\n",
    "\n",
    "        # Create vector store\n",
    "        rag.setup_vector_store(documents, \"./multi_provider_db\")\n",
    "\n",
    "        # Test question\n",
    "        question = \"What are the main topics covered in these documents?\"\n",
    "\n",
    "        # Query with Groq\n",
    "        print(\"=== GROQ RESPONSE ===\")\n",
    "        groq_response = rag.query(question)\n",
    "        print(groq_response)\n",
    "\n",
    "        # Switch to Claude\n",
    "        rag.switch_provider(\"claude\")\n",
    "        print(\"\\n=== CLAUDE RESPONSE ===\")\n",
    "        claude_response = rag.query(question)\n",
    "        print(claude_response)\n",
    "\n",
    "        # Compare both providers\n",
    "        print(\"\\n=== PROVIDER COMPARISON ===\")\n",
    "        comparison = rag.compare_providers(\"Summarize the key findings\")\n",
    "\n",
    "        for provider, result in comparison.items():\n",
    "            print(f\"\\n{provider.upper()} ({result['model']}):\")\n",
    "            print(result['response'])\n",
    "\n",
    "        # Show system info\n",
    "        print(f\"\\nSystem Info: {rag.get_system_info()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"\\nRequired environment variables:\")\n",
    "        print(\"- GROQ_API_KEY\")\n",
    "        print(\"- ANTHROPIC_API_KEY\")\n",
    "        print(\"- OPENAI_API_KEY (for embeddings)\")"
   ],
   "id": "9c6d5fee8d6b2d30",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T09:50:43.141412Z",
     "start_time": "2025-09-16T09:50:43.135382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def demo():\n",
    "    # Initialize with Groq as default\n",
    "    rag = MultiProviderRAG(\n",
    "        provider=\"groq\",\n",
    "        model_configs={\n",
    "            \"groq\": {\n",
    "                \"model\": \"llama-3.1-70b-versatile\",\n",
    "                \"temperature\": 0.1\n",
    "            },\n",
    "            \"claude\": {\n",
    "                \"model\": \"claude-3-haiku-20240307\",\n",
    "                \"temperature\": 0.2\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Load documents\n",
    "        documents = rag.load_documents([\"sample.pdf\"], source_type=\"pdf\")\n",
    "\n",
    "        # Create vector store\n",
    "        rag.setup_vector_store(documents, \"./multi_provider_db\")\n",
    "\n",
    "        # Test question\n",
    "        question = \"What are the main topics covered in these documents?\"\n",
    "\n",
    "        # Query with Groq\n",
    "        print(\"=== GROQ RESPONSE ===\")\n",
    "        groq_response = rag.query(question)\n",
    "        print(groq_response)\n",
    "\n",
    "        # Switch to Claude\n",
    "        rag.switch_provider(\"claude\")\n",
    "        print(\"\\n=== CLAUDE RESPONSE ===\")\n",
    "        claude_response = rag.query(question)\n",
    "        print(claude_response)\n",
    "\n",
    "        # Compare both providers\n",
    "        print(\"\\n=== PROVIDER COMPARISON ===\")\n",
    "        comparison = rag.compare_providers(\"Summarize the key findings\")\n",
    "\n",
    "        for provider, result in comparison.items():\n",
    "            print(f\"\\n{provider.upper()} ({result['model']}):\")\n",
    "            print(result['response'])\n",
    "\n",
    "        # Show system info\n",
    "        print(f\"\\nSystem Info: {rag.get_system_info()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"\\nRequired environment variables:\")\n",
    "        print(\"- GROQ_API_KEY\")\n",
    "        print(\"- ANTHROPIC_API_KEY\")\n",
    "        print(\"- OPENAI_API_KEY (for embeddings)\")"
   ],
   "id": "bdb10a124cf609a1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T09:50:45.935207Z",
     "start_time": "2025-09-16T09:50:45.662050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    demo()"
   ],
   "id": "e48b81812bd3892f",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChatGroq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m     \u001B[43mdemo\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 3\u001B[39m, in \u001B[36mdemo\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdemo\u001B[39m():\n\u001B[32m      2\u001B[39m     \u001B[38;5;66;03m# Initialize with Groq as default\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m     rag = \u001B[43mMultiProviderRAG\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m        \u001B[49m\u001B[43mprovider\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mgroq\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel_configs\u001B[49m\u001B[43m=\u001B[49m\u001B[43m{\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mgroq\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmodel\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mllama-3.1-70b-versatile\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtemperature\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0.1\u001B[39;49m\n\u001B[32m      9\u001B[39m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mclaude\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmodel\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mclaude-3-haiku-20240307\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtemperature\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0.2\u001B[39;49m\n\u001B[32m     13\u001B[39m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[43m        \u001B[49m\u001B[43m}\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     17\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m     18\u001B[39m         \u001B[38;5;66;03m# Load documents\u001B[39;00m\n\u001B[32m     19\u001B[39m         documents = rag.load_documents([\u001B[33m\"\u001B[39m\u001B[33msample.pdf\u001B[39m\u001B[33m\"\u001B[39m], source_type=\u001B[33m\"\u001B[39m\u001B[33mpdf\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 27\u001B[39m, in \u001B[36mMultiProviderRAG.__init__\u001B[39m\u001B[34m(self, provider, model_configs, embedding_model, chunk_size, chunk_overlap, collection_name)\u001B[39m\n\u001B[32m     24\u001B[39m \u001B[38;5;28mself\u001B[39m.model_configs = model_configs\n\u001B[32m     26\u001B[39m \u001B[38;5;66;03m# Initialize the selected LLM\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m \u001B[38;5;28mself\u001B[39m.llm = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_initialize_llm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprovider\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     29\u001B[39m \u001B[38;5;66;03m# Use OpenAI embeddings (consistent across providers)\u001B[39;00m\n\u001B[32m     30\u001B[39m \u001B[38;5;28mself\u001B[39m.embeddings = OpenAIEmbeddings(\n\u001B[32m     31\u001B[39m     model=embedding_model,\n\u001B[32m     32\u001B[39m     api_key=os.getenv(\u001B[33m\"\u001B[39m\u001B[33mOPENAI_API_KEY\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     33\u001B[39m )\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 90\u001B[39m, in \u001B[36mMultiProviderRAG._initialize_llm\u001B[39m\u001B[34m(self, provider)\u001B[39m\n\u001B[32m     87\u001B[39m config = \u001B[38;5;28mself\u001B[39m.model_configs[provider]\n\u001B[32m     89\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m provider == \u001B[33m\"\u001B[39m\u001B[33mgroq\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m90\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mChatGroq\u001B[49m(\n\u001B[32m     91\u001B[39m         model=config[\u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m     92\u001B[39m         api_key=os.getenv(\u001B[33m\"\u001B[39m\u001B[33mGROQ_API_KEY\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m     93\u001B[39m         temperature=config[\u001B[33m\"\u001B[39m\u001B[33mtemperature\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m     94\u001B[39m     )\n\u001B[32m     95\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m provider == \u001B[33m\"\u001B[39m\u001B[33mclaude\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m     96\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m ChatAnthropic(\n\u001B[32m     97\u001B[39m         model=config[\u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m     98\u001B[39m         api_key=os.getenv(\u001B[33m\"\u001B[39m\u001B[33mANTHROPIC_API_KEY\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m     99\u001B[39m         temperature=config[\u001B[33m\"\u001B[39m\u001B[33mtemperature\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m    100\u001B[39m     )\n",
      "\u001B[31mNameError\u001B[39m: name 'ChatGroq' is not defined"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
