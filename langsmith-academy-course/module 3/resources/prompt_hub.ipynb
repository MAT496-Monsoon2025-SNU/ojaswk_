{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Connecting to the Prompt Hub"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can connect our application to LangSmith's Prompt Hub, which will allow us to test and iterate on our prompts within LangSmith, and pull our improvements directly into our application."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\"  # If you don't set this, traces will go to the Default project"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T01:20:08.169191Z",
     "start_time": "2025-10-13T01:20:08.160126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../../.env\", override=True)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Pull a prompt from Prompt Hub"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Pull in a prompt from Prompt Hub by pasting in the code snippet from the UI."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T01:20:12.433576Z",
     "start_time": "2025-10-13T01:20:09.483937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "prompt = client.pull_prompt(\"pirate_man\", include_model=False)  # Changed to False"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's see what we pulled - note that we did not get the model, so this is just a StructuredPrompt and not runnable."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T01:14:39.621474Z",
     "start_time": "2025-10-13T01:14:39.618600Z"
    }
   },
   "cell_type": "code",
   "source": "prompt",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredPrompt(input_variables=['language', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': '-', 'lc_hub_repo': 'pirate_man', 'lc_hub_commit_hash': 'ab49f4279842352da4ecf9217b015ceae60e9aea06b2e8d520bb379946f8766d'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template='You are a pirate from the 1600s. You can only speak {language}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})], schema_={'title': 'answer', 'description': 'Extract the answer', 'type': 'object', 'properties': {'correctness': {'type': 'string', 'description': 'Is the submission correct, accurate, and factual?'}}, 'required': ['correctness'], 'strict': True, 'additionalProperties': False}, structured_output_kwargs={})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cool! Now let's hydrate our prompt by calling .invoke() with our inputs"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T01:20:15.970994Z",
     "start_time": "2025-10-13T01:20:15.960976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hydrated_prompt = prompt.invoke({\"question\": \"Have you found gold yet?\", \"language\": \"Spanish\"})\n",
    "hydrated_prompt"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a pirate from the future, the year 40k. You can only speak Spanish.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Have you found gold yet?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And now let's pass those messages to Anthropic and see what we get back!"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T01:14:48.246526Z",
     "start_time": "2025-10-13T01:14:42.037466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "anthropic_client = Anthropic()\n",
    "\n",
    "# Access the messages attribute directly\n",
    "messages = []\n",
    "system_message = None\n",
    "\n",
    "for msg in hydrated_prompt.messages:\n",
    "    # Get message type and content\n",
    "    if hasattr(msg, 'type'):\n",
    "        msg_type = msg.type\n",
    "        msg_content = msg.content\n",
    "    elif isinstance(msg, dict):\n",
    "        msg_type = msg.get('role', msg.get('type'))\n",
    "        msg_content = msg.get('content')\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    # Handle system messages separately\n",
    "    if msg_type == \"system\":\n",
    "        system_message = msg_content\n",
    "    else:\n",
    "        # Map LangChain role names to Anthropic role names\n",
    "        if msg_type == \"human\":\n",
    "            msg_type = \"user\"\n",
    "        elif msg_type == \"ai\":\n",
    "            msg_type = \"assistant\"\n",
    "\n",
    "        messages.append({\n",
    "            \"role\": msg_type,\n",
    "            \"content\": msg_content\n",
    "        })\n",
    "\n",
    "# Prepare system parameter\n",
    "system_param = None\n",
    "if system_message:\n",
    "    if isinstance(system_message, str):\n",
    "        system_param = [{\"type\": \"text\", \"text\": system_message}]\n",
    "    elif isinstance(system_message, list):\n",
    "        system_param = system_message\n",
    "\n",
    "# Only pass system if it's not None\n",
    "if system_param:\n",
    "    response = anthropic_client.messages.create(\n",
    "        model=\"claude-sonnet-4-20250514\",\n",
    "        max_tokens=1024,\n",
    "        system=system_param,\n",
    "        messages=messages\n",
    "    )\n",
    "else:\n",
    "    response = anthropic_client.messages.create(\n",
    "        model=\"claude-sonnet-4-20250514\",\n",
    "        max_tokens=1024,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "response"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(id='msg_01PdgEfW1q4pxrD2xiC3F6KE', content=[TextBlock(citations=None, text='¡Arr, mi capitán! No, todavía no he encontrado oro en estas aguas malditas. Hemos saqueado varios galeones españoles esta semana, pero solo hemos conseguido unas pocas monedas de plata y algunos barriles de ron. \\n\\nLos vientos no han sido favorables, y la tripulación está inquieta. Dicen que hay rumores de un tesoro enterrado en una isla cercana... ¿Qué órdenes tenéis para este humilde pirata?\\n\\n¡Por todos los demonios del mar, encontraremos ese oro dorado!', type='text')], model='claude-sonnet-4-20250514', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation=CacheCreation(ephemeral_1h_input_tokens=0, ephemeral_5m_input_tokens=0), cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=31, output_tokens=150, server_tool_use=None, service_tier='standard'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### [Extra: LangChain Only] Pulling down the Model Configuration\n",
    "\n",
    "We can also pull down the saved model configuration as a LangChain RunnableBinding when we use `include_model=True`. This allows us to run our prompt template directly with the saved model configuration."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "prompt = client.pull_prompt(\"pirate_man\", include_model=True)  # Changed to True"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T01:15:06.833348Z",
     "start_time": "2025-10-13T01:15:06.830348Z"
    }
   },
   "cell_type": "code",
   "source": "prompt",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredPrompt(input_variables=['language', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': '-', 'lc_hub_repo': 'pirate_man', 'lc_hub_commit_hash': 'ab49f4279842352da4ecf9217b015ceae60e9aea06b2e8d520bb379946f8766d'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template='You are a pirate from the 1600s. You can only speak {language}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})], schema_={'title': 'answer', 'description': 'Extract the answer', 'type': 'object', 'properties': {'correctness': {'type': 'string', 'description': 'Is the submission correct, accurate, and factual?'}}, 'required': ['correctness'], 'strict': True, 'additionalProperties': False}, structured_output_kwargs={})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Test out your prompt!"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T01:20:24.099655Z",
     "start_time": "2025-10-13T01:20:24.095595Z"
    }
   },
   "cell_type": "code",
   "source": "prompt.invoke({\"question\": \"Are you a captain yet?\", \"language\": \"Spanish\"})",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a pirate from the future, the year 40k. You can only speak Spanish.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Are you a captain yet?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Pull down a specific commit"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Pull down a specific commit from the Prompt Hub by pasting in the code snippet from the UI."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T01:20:33.198241Z",
     "start_time": "2025-10-13T01:20:32.716102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a LANGSMITH_API_KEY in Settings > API Keys\n",
    "from langsmith import Client\n",
    "client = Client()\n",
    "prompt = client.pull_prompt(\"pirate_man:18475852\", include_model=True)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Run this commit!"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T03:03:19.793394Z",
     "start_time": "2025-10-13T03:03:19.768082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "anthropic_client = Anthropic()\n",
    "\n",
    "# When using include_model=True with a specific commit, hydrated_prompt is the model output\n",
    "# We need to check what type it is first\n",
    "print(\"Type of hydrated_prompt:\", type(hydrated_prompt))\n",
    "\n",
    "# If it's a dict (already executed), you got the response directly\n",
    "# If it's a ChatPromptValue, use .messages\n",
    "\n",
    "if isinstance(hydrated_prompt, dict):\n",
    "    # This means the chain already ran and returned output\n",
    "    print(\"Already executed! Output:\", hydrated_prompt)\n",
    "else:\n",
    "    # Access messages from the ChatPromptValue object\n",
    "    messages = []\n",
    "    system_message = None\n",
    "\n",
    "    # Handle both ChatPromptValue and direct message lists\n",
    "    msg_list = hydrated_prompt.messages if hasattr(hydrated_prompt, 'messages') else hydrated_prompt\n",
    "\n",
    "    for msg in msg_list:\n",
    "        # Get message type and content\n",
    "        if hasattr(msg, 'type'):\n",
    "            msg_type = msg.type\n",
    "            msg_content = msg.content\n",
    "        elif isinstance(msg, dict):\n",
    "            msg_type = msg.get('role', msg.get('type'))\n",
    "            msg_content = msg.get('content')\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if msg_type == \"system\":\n",
    "            system_message = msg_content\n",
    "        else:\n",
    "            # Map LangChain roles to Anthropic roles\n",
    "            if msg_type == \"human\":\n",
    "                msg_type = \"user\"\n",
    "            elif msg_type == \"ai\":\n",
    "                msg_type = \"assistant\"\n",
    "\n",
    "            messages.append({\n",
    "                \"role\": msg_type,\n",
    "                \"content\": msg_content\n",
    "            })\n",
    "\n",
    "    # Format system message as required by Anthropic\n",
    "    system_param = None\n",
    "    if system_message:\n",
    "        system_param = [{\"type\": \"text\", \"text\": system_message}]\n",
    "\n",
    "    # Make the API call\n",
    "    if system_param:\n",
    "        response = anthropic_client.messages.create(\n",
    "            model=\"claude-sonnet-4-20250514\",\n",
    "            max_tokens=1024,\n",
    "            system=system_param,\n",
    "            messages=messages\n",
    "        )\n",
    "    else:\n",
    "        response = anthropic_client.messages.create(\n",
    "            model=\"claude-sonnet-4-20250514\",\n",
    "            max_tokens=1024,\n",
    "            messages=messages\n",
    "        )\n",
    "\n",
    "    # Extract and display the text content\n",
    "    response.content[0].text"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of hydrated_prompt: <class 'dict'>\n",
      "Already executed! Output: {'correctness': 'The world in the year 40k is quite different from the present day. As a pirate from the future, I can describe the world as follows:'}\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Uploading Prompts"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "You can also easily update your prompts in the hub programmatically.\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T03:08:23.924215Z",
     "start_time": "2025-10-13T03:08:21.011856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langsmith import Client\n",
    "\n",
    "client=Client()\n",
    "\n",
    "hindi_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
    "\n",
    "Your users can only speak Hindi, make sure you only answer your users with Hindi.\n",
    "\n",
    "Conversation: {conversation}\n",
    "Context: {context} \n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "hindi_prompt_template = ChatPromptTemplate.from_template(hindi_prompt)\n",
    "client.push_prompt(\"hindi-rag-prompt\", object=hindi_prompt_template)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/prompts/hindi-rag-prompt/d83fe14b?organizationId=072e35aa-3a5b-404d-bd5a-459a19c5e651'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You can also push a prompt as a RunnableSequence of a prompt and a model. This is useful for storing the model configuration you want to use with this prompt. The provider must be supported by the LangSmith playground."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T03:08:29.009644Z",
     "start_time": "2025-10-13T03:08:25.360487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langsmith import Client\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "client=Client()\n",
    "model = ChatAnthropic(model=\"claude-sonnet-4-20250514\")\n",
    "\n",
    "hindi_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
    "\n",
    "Your users can only speak Hindi, make sure you only answer your users with Hindi.\n",
    "\n",
    "Conversation: {conversation}\n",
    "Context: {context} \n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "hindi_prompt_template = ChatPromptTemplate.from_template(hindi_prompt)\n",
    "chain = hindi_prompt_template | model\n",
    "client.push_prompt(\"hindi-runnable-sequence\", object=chain)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/prompts/hindi-runnable-sequence/ce333edb?organizationId=072e35aa-3a5b-404d-bd5a-459a19c5e651'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ls-academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
