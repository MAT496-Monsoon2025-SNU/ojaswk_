{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a high-level, an evaluator judges an invocation of your LLM application against a reference example, and returns an evaluation score.\n",
    "\n",
    "In LangSmith evaluators, we represent this process as a function that takes in a Run (representing the LLM app invocation) and an Example (representing the data point to evaluate), and returns Feedback (representing the evaluator's score of the LLM app invocation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Evaluator](../../images/evaluator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a very simple custom evaluator that compares the output of a model to the expected output in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T09:57:33.268515Z",
     "start_time": "2025-10-05T09:57:33.208358Z"
    }
   },
   "source": [
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "def correct_label(inputs: dict, reference_outputs: dict, outputs: dict) -> dict:\n",
    "  score = outputs.get(\"output\") == reference_outputs.get(\"label\")\n",
    "  return {\"score\": int(score), \"key\": \"correct_label\"}"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM-as-Judge Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM-as-judge evaluators use LLMs to score system output. To use them, you typically encode the grading rules / criteria in the LLM prompt. They can be reference-free (e.g., check if system output contains offensive content or adheres to specific criteria). Or, they can compare task output to a reference (e.g., check if the output is factually accurate relative to the reference).\n",
    "\n",
    "Here is an example of how you might define an LLM-as-judge evaluator with structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set them inline\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T09:57:31.116885Z",
     "start_time": "2025-10-05T09:57:31.111944Z"
    }
   },
   "source": [
    "# Or you can use a .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../../.env\", override=True)\n",
    "os.environ[\"USER_AGENT\"] = \"496\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T09:57:37.905303Z",
     "start_time": "2025-10-05T09:57:37.700317Z"
    }
   },
   "source": [
    "from anthropic import Anthropic\n",
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "\n",
    "anthropic_client = Anthropic()\n",
    "\n",
    "class Similarity_Score(BaseModel):\n",
    "    similarity_score: int = Field(description=\"Semantic similarity score between 1 and 10, where 1 means unrelated and 10 means identical.\")\n",
    "\n",
    "# NOTE: This is our LLM-as-judge evaluator\n",
    "def compare_semantic_similarity(inputs: dict, reference_outputs: dict, outputs: dict):\n",
    "    input_question = inputs[\"question\"]\n",
    "    reference_response = reference_outputs[\"output\"]\n",
    "    run_response = outputs[\"output\"]\n",
    "\n",
    "    judge_prompt = f\"\"\"You are a semantic similarity evaluator. Compare the meanings of two responses to a question.\n",
    "\n",
    "Question: {input_question}\n",
    "\n",
    "Reference Response (correct answer): {reference_response}\n",
    "\n",
    "New Response (to evaluate): {run_response}\n",
    "\n",
    "Compare these two responses and provide a similarity score between 1 and 10:\n",
    "- 1 = completely unrelated\n",
    "- 10 = identical in meaning\n",
    "\n",
    "Even if the wording differs, focus on whether they convey the same information and answer the question in the same way.\n",
    "\n",
    "Return your response as JSON: {{\"similarity_score\": <number>}}\"\"\"\n",
    "\n",
    "    response = anthropic_client.messages.create(\n",
    "        model=\"claude-sonnet-4-5-20250929\",\n",
    "        max_tokens=300,\n",
    "        messages=[{\"role\": \"user\", \"content\": judge_prompt}]\n",
    "    )\n",
    "\n",
    "    # Parse the JSON response\n",
    "    try:\n",
    "        text = response.content[0].text\n",
    "        start = text.find(\"{\")\n",
    "        end = text.rfind(\"}\") + 1\n",
    "        result = json.loads(text[start:end])\n",
    "        similarity_score = result.get(\"similarity_score\", 5)\n",
    "    except:\n",
    "        similarity_score = 5  # Default fallback\n",
    "\n",
    "    return {\"score\": similarity_score, \"key\": \"similarity\"}"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this out!\n",
    "\n",
    "NOTE: We purposely made this answer wrong, so we expect to see a low score."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T09:56:42.192880Z",
     "start_time": "2025-10-05T09:56:38.325204Z"
    }
   },
   "source": [
    "# From Dataset Example\n",
    "inputs = {\n",
    "    \"question\": \"Is LangSmith natively integrated with LangChain?\"\n",
    "}\n",
    "reference_outputs = {\n",
    "    \"output\": \"Yes, LangSmith is natively integrated with LangChain, as well as LangGraph.\"\n",
    "}\n",
    "\n",
    "# From Run\n",
    "outputs = {\n",
    "    \"output\": \"No, LangSmith is NOT integrated with LangChain.\"\n",
    "}\n",
    "\n",
    "similarity_score = compare_semantic_similarity(inputs, reference_outputs, outputs)\n",
    "print(f\"Semantic similarity score: {similarity_score}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic similarity score: {'score': 1, 'key': 'similarity'}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also define evaluators using Run and Example directly!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T09:58:27.768082Z",
     "start_time": "2025-10-05T09:58:27.761374Z"
    }
   },
   "source": [
    "from langsmith.schemas import Run, Example\n",
    "\n",
    "def compare_semantic_similarity_v2(root_run: Run, example: Example):\n",
    "    input_question = example[\"inputs\"][\"question\"]\n",
    "    reference_response = example[\"outputs\"][\"output\"]\n",
    "    run_response = root_run[\"outputs\"][\"output\"]\n",
    "\n",
    "    user_message = f\"\"\"Compare the semantic similarity of these two responses to the question.\n",
    "\n",
    "Question: {input_question}\n",
    "\n",
    "Reference Response (correct answer): {reference_response}\n",
    "\n",
    "New Response (to evaluate): {run_response}\n",
    "\n",
    "Provide a similarity score between 1 and 10, where:\n",
    "- 1 means completely unrelated\n",
    "- 10 means identical in meaning\n",
    "\n",
    "Return your response as JSON with a single field: similarity_score (integer)\"\"\"\n",
    "\n",
    "    response = anthropic_client.messages.create(  # Changed from 'client' to 'anthropic_client'\n",
    "        model=\"claude-sonnet-4-5-20250929\",\n",
    "        max_tokens=1024,\n",
    "        system=\"You are a semantic similarity evaluator. Compare the meanings of two responses to a question, Reference Response and New Response, where the reference is the correct answer, and we are trying to judge if the new response is similar.\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Parse the response\n",
    "    import json\n",
    "    response_text = response.content[0].text\n",
    "\n",
    "    try:\n",
    "        if \"{\" in response_text and \"}\" in response_text:\n",
    "            start = response_text.find(\"{\")\n",
    "            end = response_text.rfind(\"}\") + 1\n",
    "            json_str = response_text[start:end]\n",
    "            parsed = json.loads(json_str)\n",
    "            similarity_score = parsed.get(\"similarity_score\", 5)\n",
    "        else:\n",
    "            import re\n",
    "            numbers = re.findall(r'\\d+', response_text)\n",
    "            similarity_score = int(numbers[0]) if numbers else 5\n",
    "    except:\n",
    "        similarity_score = 5\n",
    "\n",
    "    return {\"score\": similarity_score, \"key\": \"similarity\"}"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T09:59:25.282223Z",
     "start_time": "2025-10-05T09:59:22.181132Z"
    }
   },
   "source": [
    "sample_run = {\n",
    "    \"name\": \"Sample Run\",\n",
    "    \"inputs\": {\n",
    "        \"question\": \"Is LangSmith natively integrated with LangChain?\"\n",
    "    },\n",
    "    \"outputs\": {\n",
    "        \"output\": \"Yes, LangSmith is natively integrated with LangChain, as well as LangGraph.\"\n",
    "    },\n",
    "    \"is_root\": True,\n",
    "    \"status\": \"success\",\n",
    "    \"extra\": {\n",
    "        \"metadata\": {\n",
    "            \"key\": \"value\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sample_example = {\n",
    "    \"inputs\": {\n",
    "        \"question\": \"Is LangSmith natively integrated with LangChain?\"\n",
    "    },\n",
    "    \"outputs\": {\n",
    "        \"output\": \"Yes, LangSmith is natively integrated with LangChain, as well as LangGraph.\"\n",
    "    },\n",
    "    \"metadata\": {\n",
    "        \"dataset_split\": [\n",
    "            \"AI generated\",\n",
    "            \"base\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "similarity_score = compare_semantic_similarity_v2(sample_run, sample_example)\n",
    "print(f\"Semantic similarity score: {similarity_score}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic similarity score: {'score': 10, 'key': 'similarity'}\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ls-academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
